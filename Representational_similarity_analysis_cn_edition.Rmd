---
title: "表征相似性分析 (MIND 2018)"
author: "Mark A. Thornton, Ph. D."
date: "August 1, 2018"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## RSA 方法介绍

表征相似性分析（RSA）是一种基于二阶同构（second-order isomprhisms）的 fMRI 数据分析方法。这种方法并不是直接分析某个测量数据和另一个测量数据之间的关系，而是计算某个测量数据与其他数据之间的相似性，并进一步比较这些相似性数据。RSA 方法由 [Kriegeskorte, Mur, and Bandettini (2008, Frontiers in System Neuroscience)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2605405/) 在 2008 年开创，并从此成为了分析神经成像数据的流行方法。本方法之所以能够变得非常流行，是因为RSA方法使用二阶同构进行分析，这种令人惊叹的分析技术可以将不同类型的脑成像、行为数据等研究数据联系起来。

![Kriegeskorte, Mur, and Bandettini (2008)](http://www.mrc-cbu.cam.ac.uk//personal/nikolaus.kriegeskorte/fig5_kriegeskorte_RSA_FNS.gif)

在 fMRI 的分析过程中使用 RSA 方法，常常是需要在神经活动模式的相似性与任务、评分或者模型之间计算相关或者回归。在这个教程当中，我们会学习如何使用 RSA 进行验证性或探索性分析。


## 如何测量相似性？

有很多种方法可以计算不同的数据对象之间的相似性（或者是差异距离）。虽然数据的本质会约束我们选择合适的方法，我们在线性空间当中衡量距离仍然有着很多种选择。这一节我们会讨论不同的距离度量，以使我们对最终作出的选择产生直观的感受。

对于fMRI数据，我们常常用来衡量相似性的是距离是平均距离、欧式距离和相关距离。本节我们会使用一些数据来模拟这些距离，展示这些不同的距离衡量方式之间的关系。

```{r, echo=F, results="hide", include=F}
# 载入需要的包
if(!require(MASS)) install.packages("MASS"); require(MASS)
if(!require(lattice)) install.packages("lattice"); require(lattice)
if(!require(rasterVis)) install.packages("rasterVis"); require(rasterVis)
if(!require(psych)) install.packages("psych"); require(psych)
if(!require(pracma)) install.packages("pracma"); require(pracma)
if(!require(nnls)) install.packages("nnls"); require(nnls)
if(!require(smacof)) install.packages("smacof"); require(smacof)
if(!require(Rtsne)) install.packages("Rtsne"); require(Rtsne)
```

```{r}
# 生成数据
set.seed(1)
sigmat <- matrix(c(1,0,.8,0,0,1,0,.8,.8,0,1,0,0,.8,0,1),nrow=4) # 生成协方差矩阵

# 生成四组均值分别为（0，0，1，1），协方差为 sigmat 的随机数
dat <- mvrnorm(200, c(0,0,1,1), Sigma = sigmat) 

sigmat
# sigmat 是生成数据的协方差
# 所以我们知道 1与3，2与4之间存在较高相关

# 绘制生成的随机数的图形
layout(matrix(1:4,2,2))
for (i in 1:4){
  plot(dat[,i],type="o",pch=20,ylim=c(-4,4),ylab="Activity",xlab=paste("Condition",as.character(i)))
}

```

我们在这里模拟生成的数据并不符合真实情况，但是它非常适合用来呈现本问题中三种不同距离度量的差异。你可以把这四个变量看作是在一个 fMRI 实验当中，四种不同的实验条件下大脑的200个体素的激活数据。

### 平均距离（均值差异）

我们将首先开始计算平均距离 - 仅仅是这四种条件的均值的差异。这种测量方式会舍弃体素之间模式的全部信息，而且非常类似于标准的单变量 fMRI 分析。下方的棒状图和热图反映了这个均值差异。

```{r}
# 平均距离 mean distance

# 计算每个变量（数据对象）的平均值
# 每列代表一个变量（实验条件）下的数据，apply 函数使用参数 2 表示每个纵列
# 所以是对每个变量求均值

cmeans <- apply(dat,2,mean) 
barplot(cmeans) # 绘制均值
dmat1 <- as.matrix(dist(cmeans)) # calculate distance between means
levelplot(dmat1) # heatmap of distances
```

我们还记得我们刚才生成随机数的函数中的参数吧？其中第二个参数反映了生成变量的均值。因此我们可以看到第一第二变量的均值相近，第三第四变量的均值相近。故本图的含义很容易理解。

### 欧式距离

接下来我们将介绍欧式距离。欧式距离可以对应为我们常常在现实生活中所见到的“真实距离”。略微不同的是，相比于我们以前熟悉的在3维空间当中计算距离，在这里我们是要对N维空间的欧式距离进行计算。本例当中，N维空间的N是指 Voxel 的数量（即200个）。下面的热图和散点图反映了这四个条件的欧式距离。

```{r}
# 欧式距离 Euclidean distance
dmat2 <- as.matrix(dist(t(dat)))
levelplot(dmat2)
pairs(dat) # ？这样的 scatter plot 能反映距离吗？应该是在多维度上进行表征的才是距离。
```
### 相关距离
相关距离可能是在 fMRI 分析当中最常用的距离了。这种距离完全舍弃了均值（数据在求相关时已经相当于进行了 z 分数化，否则的话那就是协方差分析了）。因为相关是用来描述相似性而非不一致性的，所以“距离”这个概念我们可以简单地把相关值“翻转”一下，用 1 - R 来表示。我们再次使用热图来描述这个结果。


```{r}
# 相关距离 correlation distance
dmat3 <- 1 - cor(dat)
levelplot(dmat3)
```

### 比较距离的测量方式

现在让我们来比较一下三种距离的测量方式。在下面的热图中，我们可以看到三者之间的关系：均值和相关距离之间完全无关，但是两者对欧氏距离有着贡献。相关距离可能是最佳的选择，因为在做 RSA 方法和一般的 MVPA 方法前，常常已经完成了多变量分析。因为多变量分析本身已经能够反映平均距离，所以在做 RSA 的时候，舍去均值信息是有意义的。然而，如果你只是正在对神经相似性进行简单的探索，同时对是均值还是模式导致差异并不太过关心，那么欧式距离也许是一个好的选择。

```{r}
# combined plot
dmat2 <- dmat2/max(dmat2)
rlist <- list(raster(dmat1), raster(dmat2), raster(dmat3))
names(rlist) <- c("Mean","Euclidean","Correlation")
levelplot(stack(rlist),layout = c(3,1),at = seq(0,1,.01))
```

这个结果是否意味着使用相关来衡量神经相似性，可以使你的结果完全不受单变量分析方法的影响呢？很可惜并不是这样的。在这个简单的例子中，我们知道我们兴趣区的确切边界。然而在实际情况下，边界并不会这么清晰：激活模式与你选择的特征有关。举个例子来说，想象一下你的数据当中只有一个激活的团块，但是你的兴趣区非常的大，而这个团块只能填满这个兴趣区的 80%。剩下 20% 的体素在条件间则不会出现激活模式的改变。那么按照上面的方式进行分析，这两个体素群的差异同样会导致条件间相关性的存在——尽管在平均激活上是存在差异的。所以我再重申一下，单变量信息也许与你最后想要画的图没有关系，但是如果你的结论与非单变量分析的结果有着极强的联系，那么这对你来说也许不是一个好消息。

## RSA方法：NHST，效应量，交叉验证，模型选择

本节我们会简单介绍 RSA 方法，并且围绕着它处理一些问题。特别是，我们将会演示如何在个体水平和组水平上检验 RSA 结果的意义。我们同样会了解到 RSA 的效应量方面的一系列的注意事项，同时学到如何进行交叉验证和模型选择。

不同与我们上面所做的那个像玩具一样的例子，本节我们会使用一个真正的 fMRI 研究数据。本数据来源于过去的一项研究。 [Thornton & Mitchell, 2017, Cerebral Cortex](http://markallenthornton.com/cv/Thornton&Mitchell_CC_2017.pdf) 在本研究中，被试需要完成一项社会判断任务，对大约 60 位名人进行评分。在每次试验中，参与者都会判断特定的陈述（例如“他想要学习空手道”）对特定人物（例如 Bill Nye）的适用程度。在整个研究过程中反复重复该过程，直到对 60 个目标在 12 个项目下进行评分。在预处理之后，使用 GLM 对相同目标人物在不同实验条件下的试次进行平均。在得到每个体素上的回归参数后，对参数进行 z 分数化来排除全局背景模式的影响，再在这些参数之间求相关。这是本例开始的地方。

![Regions of reliability target person-related activity, within which we analyze patterns](http://markallenthornton.com/images/pmap.png)

首先，我们将会读取我们想要预测的神经模式的结果。这个数据由上图所示的体素当中的激活信号，经过 z 分数化后两两计算激活向量，得到相关值。

```{r}
# 读取神经数据
ndat <- read.csv("neural_pattern_similarity.csv")
dim(ndat)

# 对数据按行取平均值
nsim <- rowMeans(scale(ndat))
length(nsim)
```

我们可以看到 ndat 变量的维度是 1770 * 29，其中 1770 是条件间的 beta 值之间的得到的相关，29 是被试个数。

我们接下来则会载入一系列特征数据，是所有被试对名人在人格特质上进行的评分：

```{r fig.height=10, fig.width=8}
# 按维度读取信息
pdims <- read.csv("dimensions.csv")
pnames <- as.character(pdims$name)
pdims <- scale(pdims[,2:14])

rownames(pdims) <- pnames
levelplot(t(pdims),xlab="",ylab = "",scales = list(x = list(rot = 45)))
```

最终，我们将会读取一些整体的相似性测量信息：譬如在不同的目标名人之间的比较评级，以及通过对这些伟人的维基百科页面进行词袋模型分析，来对文本信息相似性的评估。因为这些测量的形式都是反向编码（比如距离），因此我们会反转他们来得到这些信息的相似性。

我们读入的数据中，是对所有对名人之间两两比较得到的结果。故由1770个评分。$\left(\begin{array}{c}{60} \\ {2}\end{array}\right)=\frac{60 !}{2 ! \times 58 !} = 1770$

```{r}
holdists <- read.csv("holdists.csv")
explicit <- 100 - holdists$holistic
text <- 2 - holdists$text
```

### 我们的第一个 RSA ！

现在我们已经在 R 当中载入了必要的数据，接下来让我们运行第一个 RSA 分析。这个分析将会对平均神经模式的相似性和对伟人的人格之间的相似性之间做一个相关。

```{r fig.height=7, fig.width=7}
cor(nsim,explicit)

plot(explicit,nsim,xlab = "Rated similarity",ylab = "Neural similarity",pch = 20)
abline(lm(nsim~explicit),col = "red",lwd = 2)
```

正如你所看到的，在人们想象那些名人如何看待其他人，与人们在想像这些人时的脑功能活动模式的差异之间，存在着明显的相关性。皮尔逊相关得到的r值为 0.4 。（Kriegeskorte 和 Colleagues 建议使用斯皮尔曼相关来代替皮尔逊相关。他们的建议很有道理。但是我在实际的研究中，很少见到方法上的差异所导致的结果的不同。我们这里来尝试一下。

```{r}
cor(nsim,explicit,method = "spearman")

```

所以你能够看到，这就是表征相似性分析的基本概念。无论是脑成像的结果，还是评分的结果，这两者都是对实验中刺激（60位伟人）的表征。而后续进行的分析就是对这两个表征之间进行的比较。

### Null hypothesis significance testing

How can we tell if this correlation is statistically significant? A parametric correlation test assumes that every observation is independent, and will thus give us a fixed-effects p-value.

我们如何判断这个相关是否统计显著呢？做一个独立样本相关参数检验能够给我们一个固定效应的 p 值。

```{r}
cor.test(nsim,explicit)
```

We can see that the results of this test are wildly significant (i.e., p = 0 to within machine precision). However, if are stimuli are just s small sample from a large population, we may want a p-value that reflects the dependencies in the similarity matrices. To achieve this, we can instead use a permutation test. As with any permutation test, it is important that we permute at the level of independent observations. In this case, that means permuting the rows and columns of one of our similarity matrices with respect to the other.

我们能够看到检验的结果非常的显著。然而，如果我们的模拟练习仅仅是从大量数据当中获取的一个小样本，我们也许希望结果是一个能够反映相似性矩阵依赖性的 p 值。为了实现这一目标，我们可以使用置换检验。正如我们平常使用的置换检验一样，在独立样本水平上进行置换十分重要。在本例当中，这意味着我们需要对不同的相似性矩阵之间的行列之间进行互换。

```{r}
# 将向量化的神经元相似性矩阵转换回矩阵
sqnsim <- squareform(nsim)

# 为可重复性分析设置随机数
set.seed(1)
nperm <- 5000 # 设置置换次数
nppl <- dim(sqnsim)[1] # 实验中的刺激物（伟人）个数
permcor <- rep(NA,nperm) # 定义变量
for (i in 1:nperm){
  sel <- sample(nppl) # 置换向量（选择第几行/第几个伟人）
  rnsim <- squareform(sqnsim[sel,sel]) # 置换矩阵并重新向量化
  permcor[i] <- cor(rnsim,explicit)    # 重新计算相似性
}

# 重新计算相似性的p值
mean(abs(permcor) > cor(nsim,explicit))

# 对p值的结果进行可视化
hist(abs(permcor),xlim=c(0,.4),main="Permuted null versus actual correlation")
abline(v=cor(nsim,explicit),col="red",lwd=2)

```

正如你所看到的，在本例当中，我们的置换检验结果显示，p值大部分仍然保持在0左右。但是请记住的是大多数情况并不和本例一样。

置换检验解决了结果是否依赖于相似性矩阵的形态这一问题，但是我们所做的相关性分析仍然在只在项目水平上是有效的（因为我们在不同的被试间平均了神经相似性）。因此，我们获取的p值严格来说是对被试样本的推断，而是对实验中呈现的名人的样本的推断。为了得到更一般的对应于社会群体的结果，我们需要进行一个随机效应分析。最简单的方法是进行一个摘要分析。

```{r}
# 对每一个被试进行相关分析
ncors <- cor(explicit,ndat)

# 对参数进行单个样本t检验 (注意求一个双曲函数)
t.test(atanh(ncors))
```

在上面的例子中，我们对每个被试的神经相似性和伟人的比较评分之间求了相关。随后我们对结果做了一个单样本t检验，来判断均值是否大于零。注意这个时候算出的相关值是非线性且不服从正态分布的，所以我们通过费舍尔 z 转换使得相关值符合参数检验的先验条件。如果通过转换也没有办法获得一个符合 NHST 前提的数据，我们同样可以像下面一样通过 bootstrapping 方法做非参数检验。

```{r}
# bootstrap 95% CI
bootres <- replicate(5000,mean(sample(ncors,length(ncors),T)))
quantile(bootres,c(.025,.975))

# visualize result
plot(1:29,sort(ncors),xlab="Participant (sorted)",ylab="Correlation (r)",
     pch=20,xlim=c(1,30))
points(30,mean(ncors),col="red",cex=2,pch=20)
segments(30,quantile(bootres,.975),30,quantile(bootres,.025),col="red",lwd=2)
abline(h=0)
```

Note that the correlation is greater than zero for all but one participant in this data set, so formal NHST is scarcely necessary to reject the null hypothesis. 

注意几乎所有的相关值都大于0，只有一个被试的相关值是小于0的，因此 NHST 无需拒绝零假设。

尽管在组水平上分析数据能够让我们来推断总体，但是最佳的 NHST 方法应该能让我们对被试和项目层面都能进行随机效应分析（对所有对自变量的随机截距和随机斜率都进行分析）。然而不幸的是，由于相似性矩阵的随机效应结构的复杂性，最佳的最大随机效应模型非常难以收敛。因此对于大多数的 RSA 分析来说，几乎不可能得到最为正确的 NHST 结果。然而还有一些备选方案，比如 [感染率推断](https://arxiv.org/abs/1512.00810) 。另一种选择是使用贝叶斯统计，因为贝叶斯混合效应模型有时能够收敛最小二乘法无法收敛的模型。这些选择已经超出了本教程所要介绍的范围，但是后面我们会谈一下另一种解决这个问题的方式：交叉验证。

### 效应量

Effect sizes in RSA are relatively straight-forward. For item-level analyses like the first one we examined, the effect size is simply the correlation between the two similarity measures. However, it's worth bearing in mind the reliability of your measures when you do so. Neural data can often be quite noisy, and this can make it appear as if the association between neural similarity and another measure is smaller than it really is. One way to deal with this is through correlation disattenuation:

```{r}
# compute raw correlation
cor(nsim,explicit)
# compute inter-participant Cronbach's alpha for neural data
rel <- alpha(ndat)
# standardized alpha
rel$total$std.alpha
# compute disattenuated correlation
cor(nsim,explicit)/sqrt(rel$total$std.alpha)
```

In this example, the correlation between neural and explicit similarity increased from 0.4 to 0.56 when we adjusted for the mediocre reliability of the neural data. The same could be done for the ratings, if we had the individual participant data from which to calculate its reliability. Note only the raw correlation should be used for NHST purposes. Also, remember that the reliability we calculated is only an estimate of the true variance in the data. When that estimate itself is noisy (e.g., with small samples) it clean lead to nonsensical results, like correlations greater than one. Thus, although this approach can be helpful, it should be applied with caution.

There is another reason besides reliability why RSA correlations might be smaller than they should be: converting raw data into a similarity matrix inherently discards information. As a result, on average an RSA correlation will be the square-root of the correlation in the underlying dimensions used to create the similarity matrices.

![](http://markallenthornton.com/images/rsacor.png)

If you care about variance-explained in similarities themselves, then this isn't really a problem. However, if you care more about variance in the variables underlying your similarity estimates, you could arguably improve you effect size estimates by taking the square root of the RSA correlations you obtain.

#### Group level effect sizes

At the group level, we can calculate familiar test-statistics like Cohen's d.

```{r}
# Compute Cohen's d
mean(atanh(ncors))/sd(atanh(ncors))
```

Here, for instance, we can see that the Cohen's d for the association between explicit similarity ratings and neural similarity is 2.16. The standard rule of thumb for Cohen's d states that a "large effect" is 0.8, so at 250%+ of that, we've got a whopper here!

It's also possible to estimate a wide-range of other standardized effect size measures in the context of RSAs based multiple regressions or mixed effects models. However, those measures aren't RSA-specific, so won't consider them here.

### Cross-validation

Significance testing offers some nice hypothetical guarantees about inference to a population. However, cross-validation offers a much more concrete way of assessing how well your model can actually predict new data. In cross-validation, a model is trained on some subset of data, and then tested on left-out data, iteratively treating each subset as test and training set. Let's conduct a cross-validated multiple regression RSA, predicting neural similarity from explicit ratings and text!

```{r fig.height=7, fig.width=7}
# plot item-level scatterplot matrix
pairs(data.frame(neural=nsim,explicit,text),pch=20)

# rescale neural similarities to positive values
ndatp <- ndat-min(ndat)

# conduct leave-one-participant-out cross-validation
xvars <- cbind(1,explicit,text)
nsub <- dim(ndat)[2]
cvperf <- rep(NA,nsub)
for (i in 1:nsub){
  fit <- nnls(xvars,rowMeans(ndatp[,-i])) # fit using non-negative least squares
  cvperf[i] <- cor(fit$fitted,ndatp[,i])
}
mean(cvperf)

```
Here we can see that together, these two predictors can achieve a cross-validated performance of r = .11. Note that there are many other measures for assessing such performance, such as RMSE (root-mean-square-error), but correlation is a relatively interpretable choice. However, how can we contextual this performance? An r = .11 is obviously not great in absolute terms, but what's the best we could have done? We can answer this by calculating a noise ceiling. In this context, that simply means the average correlation between each participant and the average of the other participants.

```{r fig.height=7, fig.width=7}
noise <- rep(NA,nsub)
for (i in 1:nsub){
  noise[i] <- cor(ndat[,i],rowMeans(ndat[,-i]))
}
mean(noise) # noise ceiling
mean(cvperf)/mean(noise) # performance as fraction of noise ceiling
```

The noise ceiling for this data is only r = 0.129. This means that, given the amount of heterogeneity between participants, even a hypothetical perfect model couldn't do better than r = 0.129. If we divide our actual performance by the noise ceiling, we can see that together, explicit ratings and textual similarity achieve 86% of the performance of a perfect theory. Although the calculations are somewhat different due to the cross-validation, you might realize that what we've just done is conceptually the same as the correlation disattenuation we performed earlier. Thus, although noise ceilings can be helpful, they should be interpreted with a grain of salt because they are only estimates and they carry their own assumptions with them. For instance, in the example above, we're implicitly assuming that all meaningful variance is at the group level (i.e., there are no meaningful idiosyncrasies), which is almost certainly not true. 

More generally, using different cross-validation schemes can be very useful for estimating generalization across different "boundaries" in your data. In the example above, we only cross-validated with respect to participants, but in the case below, we also cross-validate with respect to mental states.

```{r}

# define function to translate vector selector to matrix selector and vectorize
rsasel <- function(selvec){
  nobj <- length(selvec)
  selmat <- matrix(F,nobj,nobj)
  selmat[selvec,selvec] <- T
  diag(selmat)<-0
  return(squareform(selmat))
}


# split data by target people
set.seed(1)
targsel <- sample(c(rep(T,30),rep(F,30)))
targsel1 <- rsasel(targsel)==1
targsel2 <- rsasel(!targsel)==1
ndatp1 <- ndatp[targsel1,]
ndatp2 <- ndatp[targsel2,]
xvars <- cbind(1,explicit,text)
xvars1 <- xvars[targsel1,]
xvars2 <- xvars[targsel2,]
  
  
# conduct leave-one-participant-out and split-half-by-target cross-validation 
nsub <- dim(ndat)[2]
cvperf <- matrix(NA,nsub,2)
for (i in 1:nsub){
  fit <- nnls(xvars1,rowMeans(ndatp1[,-i]))
  cvperf[i,1] <- cor(xvars2 %*% fit$x,ndatp2[,i])
  fit <- nnls(xvars2,rowMeans(ndatp2[,-i]))
  cvperf[i,2] <- cor(xvars1 %*% fit$x,ndatp1[,i])
}
mean(cvperf)

```

The performance of the model dropped to r = .108 (vs r = .111) when we ask it to generalize to a new set of target people. However, this is a very slightly reduction in performance, suggesting that this model could potentially generalize quite well to new target people. In practice, we should probably repeat the process above multiple times with different split halves (or another cross-validation scheme) to ensure that the results we see aren't a fluke. Note that cross-validation can thus give us an answer which is difficult to obtain through significance testing (since, as discussed previously, it can be difficult to get a maximal mixed effects model to converge for such data).

### Model selection

In the examples so far, we've worked with models that have only one or two predictors. In many studies, researchers try to explain neural similarity using many more dimensions. As models get more complex, how can we tell when we have the best RSA model to explain brain activity? Model selection is not a unique problem for RSA, but it is an important one. There are many good ways to pick the best set of dimensions for a regression model. Traditional methods include stepwise and best subset regression. However, here we'll give an example of a simple exhaustive search through a relatively small set of dimensions.

Among the various predictors we asked participants to rate famous people on are the cardinal dimensions of the Big 5: openness, conscientiousness, extraversion, agreeableness, and neuroticism. What combination of these five predictors is best for explaining
the neural pattern similarity between representations of famous people?

```{r}
# select appropriate dimensions from rating matrix
big5 <- pdims[,c(2,5,8,10,11)]
big5d <- -apply(big5,2,dist) # convert to distances and sign flip
big5d <- big5d - min(big5d) # ensure positivity

# enumerate all possible combinations
b5combs <- list()
ind <- 1
for (i in 1:5){
  ccombs <- combn(5,i)
  for (j in 1:dim(ccombs)[2]){
    b5combs[[ind]] <- ccombs[,j]
    ind <- ind + 1
  }
}

# cycle through combinations, with LOO-CV
nc <- length(b5combs)
perf <- matrix(NA,nc,nsub)
for (i in 1:nc){
  xvars <- cbind(1,big5d[,b5combs[[i]]])
  for (j in 1:nsub){
    fit <- nnls(xvars,rowMeans(ndatp[,-j])) # fit using non-negative least squares
    perf[i,j] <- cor(fit$fitted,ndatp[,j])
  }
}
# calculate model with best performance
mperf <- rowMeans(perf)
colnames(big5)[b5combs[[which(mperf==max(mperf))[1]]]]

```

The best model consists of just 3 of the Big 5: openness, conscientousness, and extraversion. In practice, if we wanted to then assess the performance of this model (e.g., relativel to the noise ceiling), we would be well-advised to separate the model selection and evaluation steps. In this case, we were only examining 31 possible models (combinations of 5 variables) but the space of possible models grows very rapidly as the number of possible predictors increases. This means that with just a few more dimensions, we would be at serious risk of overfitting by model selection (rather than the typical case of model selection via parameter estimation). Using independent data for model selection and evaluation mitigates this risk.

## Exploration and visualization

So far we have mainly examined representational similarity from a confirmatory perspsective: trying to fit particular models to the observed neural data. However, exploratory analysis - including data visualization - is essential to complement these confirmatory approaches. In this section we will explore several important exploratory and visualization techniques.

### Multidimensional scaling

Multidimensional scaling (MDS) is one of the most powerful techniques for visualizing and exploring similarity data. The basic idea is to take a distance matrix and use it to reconstruct a configuration of points that embody those distances as accurately as possible within an N-dimensional space. In practice, N usually is 2 (sometimes 3) since this makes the results easy to visualize. There are many implementations in R, but we'll be using the smacof package.

#### Trival example: airline distances between French cities

```{r fig.height=8, fig.width=8}
layout(mat=matrix(1))
mdfit <- smacofSym(Guerry,2)
plot(mdfit) # a reconstructed map of France!
```
Note that the orientation of an MDS plot is arbitrary: this is one of the major limitations of the method. However, different configurations can be rotated into the same orientation via Procrutes algorithm.

#### MDS on neural pattern similarity
Now that we've seen how MDS works, let's apply it to our fMRI data.
```{r fig.height=8, fig.width=8}
# name similarity matrix for plotting
rownames(sqnsim) <- pnames
colnames(sqnsim) <- pnames

# flip sign and make positive
psqnsim <- -sqnsim
psqnsim <- psqnsim - min(psqnsim)

# fit MDS
mdfit1 <- smacofSym(psqnsim,2)
plot(mdfit1)

```

The closer to people are in the figure above, the more similar the neural pattern they elicit. How can we tell if this configuration is a good fit for the data? The key statistic for assessing MDS fit is called "stress". It's a measure of how far each point is in the configuration relative to where it "should" be, given the input distance matrix. A typical rule of thumb is "stress > .15 = bad" but this is not really a good approach because stress depends heavily on the number of 'objects' in the distance matrix. Stress per point (SPP) is more helpful, as it can illustrate which particular points are ill-fit.

```{r fig.height=8, fig.width=8}
mdfit1$stress
plot(mdfit1,plot.type="stressplot")

```

The overall stress of this configuration is pretty bad. Nancy Grace, Michael Jordan, Jimmy Fallon, and Bob Marley are particularly 'misplaced' in the configuration plot, relative to their actual neural similarity to other targets. Let's try to get a better fit! One way to do this is to relax the assumption that your similarity measure is a metric variable, and instead treat it as ordinal. 

```{r fig.height=4, fig.width=8}

# fit MDS
mdfit2 <- smacofSym(psqnsim,2,"ordinal")
mdfit2$stress

# Shepard plots are another helpful visual diagnostic to examine fit
layout(mat = t(matrix(c(1,2))))
plot(mdfit1,plot.type="Shepard","Ratio fit")
plot(mdfit2,plot.type="Shepard","Ordinal fit")
```

Unfortunately, switching to an ordinal MDS hasn't helped much - our data are just too high dimensional to reproduce their similarities on a 2-dimensional manifold! In a moment, we'll take a look at a different manifold-learning technique that might fair a bit better. However, let's first examine one nice addition to MDS: biplots. As I mentioned earlier, the axes of an MDS are arbitrary. Biplots are arrows added to an MDS which indicate the correlation of each dimension with the axes of the plot. They help to interpret the meaning behind the MDS plot.

![A heavily embellished MDS, including biplots](http://markallenthornton.com/images/person-space.png)

#### t-SNE 
t-SNE, or t-distributed stochastic neighbor embedding, is another popular manifold learning/dimensionality reduction algorithm. Where MDS tries to preserve the global structure of the data, t-SNE ties to preserve the local structure. Thus long-range distances in an MDS configuration are quite meaningful, but you're not guaranteed to end up with true nearest-neighbors being represented as such in the configuration. In contrast, in t-SNE the long range distances don't mean much, but nearest neighbor structure will be preserved as much as possible.

```{r fig.height=8, fig.width=8}

# fit t-SNE and plot
set.seed(1)
tres <- Rtsne(as.dist(psqnsim),perplexity=4)
plot(tres$Y,pch=20,xlab="",ylab="")
text(tres$Y[,1],tres$Y[,2]+2,pnames,cex=.75)
```

You can see here that the results of t-SNE tend to look a bit more cluster-like. How much this is true depends on the "perplexity" parameter of the algorithm. This parameter can be difficult to set because different settings reveal different structure in the data. There is an excellent introduction to setting perplexity and interpreting t-SNE can be found [here](https://distill.pub/2016/misread-tsne/).

#### Hierarchical clustering
Clustering is another powerful technique for exploring similarity data. Clustering is a complex topic of its own, so we won't go into too much detail here, but let's take a quick look at one method: hierarchical clustering. It can take two main forms: agglomerative (bottom-up) merging of points into larger and larger cluster, and divisive (top-down) breaking of clusters eventually yielding individual points. Both forms operate directly on distance matrices rather than data matrices, and both yield the familiar tree-like structure of dendrograms. Agglomerative clustering tends to be faster and more common, so we'll focus on it. Hierarchical clustering is particularly valuable for data exploration because it pairs well with a powerful visualization technique - the dendrogram. 

```{r fig.height=10, fig.width=8}

hc <- hclust(as.dist(psqnsim),method = "ward.D2")
plot(as.dendrogram(hc),horiz=T,xlim=c(4,-1))
```

## Conclusion

Today we've learned how to conduct representational similarity analyses of fMRI data. First, we learned several methods for calculating the similarity between data objects. Next, we learned about RSA proper, including methods for NHST, effect sizes, cross-validation, and model selection. Finally, we learned about several techniques for exploring and visualizing similarity data, including multidimensional scaling, t-SNE, and hierarchical clustering. Together these methods should give you an excellent position from which to start analyzing neural similarity data!


